<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Abhishek Kumar</title>

    <!-- Bootstrap Core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>

    <!-- Plugin CSS -->
    <link href="vendor/magnific-popup/magnific-popup.css" rel="stylesheet">

    <!-- Theme CSS -->
    <link href="css/creative.min.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

    </head>

    <body id="page-top">

        <nav id="mainNav" class="navbar navbar-default navbar-fixed-top">
            <div class="container-fluid">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                        <span class="sr-only">Toggle navigation</span> Menu <i class="fa fa-bars"></i>
                    </button>
                    <a class="navbar-brand page-scroll" href="#page-top"><font color="#FF5733">Abhishek Kumar</font></a>
                </div>

                <!--<p class="navbar-text">&#21325;</p>-->
                <!-- Collect the nav links, forms, and other content for toggling -->
                <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a class="page-scroll" href="#about"><font color="#000000">About</font></a>
                        </li>
                        <li>
                            <a class="page-scroll" href="#technologies"><font color="#000000">Skills</font></a>
                        </li>
                        <li>
                            <a class="page-scroll" href="#portfolio"><font color="#000000">Projects</font></a>
                        </li>
                        <li>
                            <a class="page-scroll" href="#cv"><font color="#000000">Resume</font></a>
                        </li>
                        <li>
                            <a class="page-scroll" href="#contact"><font color="#000000">Contact</font></a>
                        </li>
                    </ul>
                </div>
                <!-- /.navbar-collapse -->
            </div>
            <!-- /.container-fluid -->
        </nav>

        <header>
            <div class="header-content">
                <div class="header-content-inner">
                    <h1 id="homeHeading">Hey.</br> Glad, You're</br>Here !</h1>
                    <hr>
                    <!--<p>Start Bootstrap can help you build better websites using the Bootstrap CSS framework! Just download your template and start going, no strings attached!</p>-->
                    <p><h2><font color="#FFFFFF">Meet a problem solver. My inspiration: </font></h2></p>
                    <!--<p><h2><font color="#FFFFFF">$ echo "Learn Understand And Strike"</font></h2></p>-->
                    <p><h3><font color="#FFFFFF"><strong>"If you're satisfied, you cannot grow,</br>
                        If you cannot grow, you cannot be your best,</br>
                        If you cannot be your best, you cannot be happy,</br>
                        And if you're not happy, what else matters..."</strong>
                    </font></h3></p>
                    
                    <a href="#about" class="btn btn-primary btn-xl page-scroll">Find Out More</a>
                </div>
            </div>
        </header>

        <section class="bg-dark" id="about">
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2 text-center">
                        <h2 class="section-heading">$ whoami</h2>
                        <hr class="light">
                        <!--<p class="text-faded">Start Bootstrap has everything you need to get your new website up and running in no time! All of the templates and themes on Start Bootstrap are open source, free to download, and easy to use. No strings attached!</p>-->
                        <p>I'm Abhishek, a graduate student in Computer Science at Arizona State University. My research interests fall in the general area of theory and applications of machine learning, computer vision. Other area which intersts me is Data Science, Cloud computing, Systems.</p>
                        <p> I am a Graduate Research Assistant at ASU working on compression of weights in a Recurrent Neural Network. This will help to save memory on hardwares.</p>
                        <p>Prior to my Master's, I worked for 4 years as SDE at Samsung and SMTS at Mentor Graphics. I implemented features and applications in Android Systems and Electronic Design Automations. I hold a Bachelor's degree in Computer Science from Birla Institute Of Technology, Mesra, India.</p>
                        <p>Volunteer Experience: I'm a mentor at 'The Arizona Mentor Society' (AMS) mentoring 5th graders at a level one elementary school in Tempe.</p>
                        <!--For more details, please see <a class="scrollto" href="#cv"><font color="#FF5733">my Resume</font></a>.-->
                        <div class="call-to-action text-center"><a class="btn btn-primary btn-xl sr-button" href="#cv"> View my Resume</a>   &nbsp;&nbsp;&nbsp;OR&nbsp;&nbsp;&nbsp;  <a href="https://www.edocr.com/web-api/shares/y9xoraz4/download?redirect=false" class="btn btn-primary btn-xl sr-button">Download Resume</a></div>
                    </p>
                    <!--<a href="#technologies" class="page-scroll btn btn-default btn-xl sr-button">Get Started!</a>-->
                </div>
            </div>
        </div>
    </section>

    <section class="bg-dark" id="technologies">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">$ ls skills</h2>
                    <h3>So, what can I bring to the table <i class="fa fa-1x fa-th-list text-primary sr-icons"></i></h3>
                    <h4>*Click on icons for details.</h4>
                    <hr class="primary">
                </div>
            </div>
        </div>
        <div class="container">
            <div class="row">
                <div class="col-lg-3 col-md-6 text-center">
                    <div class="service-box">
                        <a href="#" data-toggle="modal" data-target="#tech-know"><i class="fa fa-4x fa-book text-primary sr-icons"></i></a>
                        <font color="white"><h3>Knowledge</h3>
                            <p class="text-muted">Summary of my technical skills and the courses I took.</p></font>
                        </div>
                    </div>
                    <div class="col-lg-6 col-md-6 text-center">
                        <div class="service-box">
                            <a href="#" data-toggle="modal" data-target="#work_exp"><i class="fa fa-4x fa-briefcase text-primary sr-icons"></i></a>
                            <h3>Work Experience</h3>
                            <a href="#" data-toggle="modal" data-target="#work_exp"><center><img src="img/work_exp/samsung.jpg" class="img-responsive" alt="" height="90" width="160"></center></a>
                        </br>
                        <a href="#" data-toggle="modal" data-target="#work_exp"><center><img src="img/work_exp/mentor.jpg" class="img-responsive" alt="" height="60" width="120"></center></a>
                        <!--
                        <center><img src="img/work_exp/samsung.jpg" class="img-responsive" alt="" height="90" width="180"></center>
                        </br></br>
                        <center><img src="img/work_exp/mentor.jpg" class="img-responsive" alt="" height="70" width="140"></center>
                    -->
                </div>
            </div>
            <div class="col-lg-3 col-md-6 text-center">
                <div class="service-box">

                    <a href="#" data-toggle="modal" data-target="#passion"><i class="fa fa-4x fa-star text-primary sr-icons"></i></a>
                    <!--<i class="fa fa-4x fa-newspaper-o text-primary sr-icons"></i>-->
                    <h3>Passion</h3>
                    <p class="text-muted">I give my 100% to what I do.</p>
                </div>
            </div>
                <!--
                <div class="col-lg-3 col-md-6 text-center">
                    <div class="service-box">
                        <i class="fa fa-4x fa-heart text-primary sr-icons"></i>
                        <h3>Made with Love</h3>
                        <p class="text-muted">You have to make your websites with love these days!</p>
                    </div>
                </div>
            -->
        </div>
    </div>
</section>

<!-- technical knowledge -->
<div id="tech-know" class="modal fade" role="dialog">
    <div class="modal-dialog modal-lg">
        <!-- Modal content-->
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal">&times;</button>
                <h4 class="modal-title">Knowledge</h4>
            </div>
            <div class="modal-body">
                <div class="panel-group" id="accordionu" role="tablist" aria-multiselectable="true">
                    <div class="panel panel-default">
                        <div class="panel-heading" role="tab" id="headingOneu">
                            <h4 class="panel-title">
                                <a class="collapsed" role="button" data-toggle="collapse" data-parent="#accordionu" href="#collapseOneu" aria-expanded="false" aria-controls="collapseOneu">
                                    <i class="fa fa-2x fa-yelp text-primary sr-icons"></i>&nbsp;Technical Skills and Knowledge
                                </a>
                            </h4>
                        </div>
                        <div id="collapseOneu" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="headingOneu">
                            <div class="panel-body">
                                <h3><strong>Programming</strong></h3>
                                <ul>
                                    <li>C/C++, Python, Java, SQL, MySQL Workbench, MATLAB, JavaScript, Django, HTML, Latex</li>
                                </ul>
                                <h3><strong>Frameworks/Libraries</strong></h3>
                                <ul>
                                    <li>numpy, TensorFlow, YANN (Theano), scikit-learn, Git, Android, Visual Studio, Eclipse</li>
                                </ul>
                                <h3><strong>Deep Learning</strong></h3>
                                <ul>
                                    <li>CNN, GAN, RNN, LSTM, Auto-encoder, Regressions, Word2Vec etc.</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="panel panel-default">
                        <div class="panel-heading" role="tab" id="headingTwou">
                            <h4 class="panel-title">
                                <a class="collapsed" role="button" data-toggle="collapse" data-parent="#accordionu" href="#collapseTwou" aria-expanded="false" aria-controls="collapseTwou">
                                    <i class="fa fa-2x fa-graduation-cap text-primary sr-icons"></i> Masters in Computer Science. University Courses: 
                                </a>
                            </h4>
                        </div>
                        <div id="collapseTwou" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwou">
                            <div class="panel-body">

                                <h3><strong>Arizona State University - Ira A. Fulton Schools of Engineering | GPA: 3.83 | May, 2018</strong></h3>
                                <ul>
                                    <li>Statistical Machine Learning <small> (CSE 575)</small></li>
                                    <li>Fundamentals of Statistical Learning <small> (CSE 569)</small></li>
                                    <li>Introduction to Deep Learning in Visual Computing <small> (CSE 591)</small></li>
                                    <li>Natural Language Processing <small> (CSE 576)</small></li>
                                    <li>Multimedia and Web Databases <small> (CSE 515)</small></li>
                                    <li>Mobile Computing <small> (CSE 535)</small></li>
                                </ul>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading" role="tab" id="headingThreeu">
                                <h4 class="panel-title">
                                    <a class="collapsed" role="button" data-toggle="collapse" data-parent="#accordionu" href="#collapseThreeu" aria-expanded="false" aria-controls="collapseThreeu">
                                        <i class="fa fa-2x fa-graduation-cap text-primary sr-icons"></i> Bachelor Of Engineering in Computer Science. University Courses: 
                                    </a>
                                </h4>
                            </div>
                            <div id="collapseThreeu" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThreeu">
                                <div class="panel-body">

                                    <h3><strong>Birla Institute Of Technology, Mesra, India | GPA: 7.13 | May, 2012.</strong></h3>
                                    <h4>Some important courses I took at BIT.</h4>
                                    <ul>
                                        <li>Artificial Intelligence <small> (CP 8101)</small></li>
                                        <li>Compiler Design <small> (CP 6105)</small></li>
                                        <li>Computer Graphics<small> (CP 7103)</small></li>
                                        <li>Computer Networks <small> (CP 6107)</small></li>
                                        <li>Data Mining and Warehousing <small> (IT 8101)</small></li>
                                        <li>Operating Systems <small> (CP 4107)</small></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="modal-footer">
                <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
            </div>
        </div>
    </div>
</div>

<!-- Work Experience -->
<div id="work_exp" class="modal fade" role="dialog">
    <div class="modal-dialog modal-lg">
        <!-- Modal content-->
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal">&times;</button>
                <h4 class="modal-title">Work Experience</h4>
            </div>
            <div class="modal-body">
                <div class="panel-group" id="accordion" role="tablist" aria-multiselectable="true">

                    <div class="panel panel-default">
                        <div class="panel-heading" role="tab" id="headingOne">
                            <h4 class="panel-title">
                                <a class="collapsed" role="button" data-toggle="collapse" data-parent="#accordion" href="#collapseOne" aria-expanded="false" aria-controls="collapseOne">
                                    Mentor Graphics
                                </a>
                            </h4>
                        </div>
                        <div id="collapseOne" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="headingOne">
                            <div class="panel-body">
                                <h3><strong>Senior Member Technical Staff | 2015-16 | Noida, India</strong></h3>
                            <!--<div class="pull-right boxed"><time>Aug 2015 - Jul 2016 </time></div></br>
                            <small><i>Noida, India</i></small>-->
                            <ul>
                                <li>Lead the module, Velcomp which is a system driver to perform compilation of electronic system design for Veloce. Implemented new features and solved issues on it.</li>
                                <li>Debugged issues in Net Delay graph at FPGA level for routing shortest path within the chip network.</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="panel panel-default">
                    <div class="panel-heading" role="tab" id="headingTwo">
                        <h4 class="panel-title">
                            <a class="collapsed" role="button" data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">
                                Samsung
                            </a>
                        </h4>
                    </div>
                    <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
                        <div class="panel-body">
                            <h3><strong>Software Developer | 2012-15 | Noida, India</strong></h3>
                            <ul>
                                <li>Modelled a novel idea on Automatic Power Saving Mode for Samsung devices.</li>
                                <li>Optimized ART (Android Runtime) for new releases in Android and solved several issues in SSRM (Samsung Smart Resource Management). Through SSRM we control the AP temperature of the devices and in ART, we work upon initial boot-timing optimization of the devices. My role was to work on several issues, provide new ideas and solutions to optimize these modules.</li>
                                <li>Implemented a tool which converts one version of SSRM to another(XML Parsing is involved). This was highly helpful for High and Low end models in OS upgrades. </li>
                                <li>Implemented memory solutions like Runtime Compcache, DHA etc. </li>
                                <li>Other roles include working on several issues and implementation of memory optimization solution to improve the device sluggishness. </li>
                                <li>Worked on various booting issues at kernel level and board support packages.</li>
                                <li>Training on system and performance in Samsung HQ,South Korea.</li>
                            </ul>
                        </div>
                    </div>
                </div>

            </div>
        </div>
        <div class="modal-footer">
            <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
        </div>
    </div>
</div>
</div>

<!-- passion Modal -->
<div id="passion" class="modal fade" role="dialog">
    <div class="modal-dialog modal-lg">
        <!-- Modal content-->
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal">&times;</button>
                <h4 class="modal-title">Passion</h4>
            </div>
            <div class="modal-body">

                <div class="panel panel-default">
                    <div class="panel-heading" role="tab" id="headingOne">
                        <h3 class="panel-title">
                            Enjoy Life in my own way. My wish : <strong>To be better today than yesterday </strong>
                        </h3>
                    </div>
                    <div class="panel-body">

                        <ol>
                            <li> I'm a big cricket fan, following the game for the last 18 years. I don't want to miss a single match that India plays. <a href="https://www.youtube.com/watch?v=mnN_UPCpWhg" target="_blank"> World Cup Champion, 2011.</a></li>
                            <li> I am a foodie, traveller and a noob in photography (The picure in my home page has been clicked by me near Flagstaff, Arizona). </li>
                            <li> I also love solving problems on <a href="https://leetcode.com">Leetcode</a>. You can look at some of the problems I solved. Its available on <a href="https://github.com/luas13/Leetcode_New">Github</a>. <center> <a href="https://github.com/luas13/Leetcode_New"> <img src="img/other_sites/leetCode.jpg" class="img-responsive" alt="" width="60" height="60"></center></li>
                            <li> I also love solving problems on <a href="https://leetcode.com">Leetcode</a>. You can look at some of the problems I solved. Its available on <a href="https://github.com/luas13/Leetcode_New">Github</a>. <center> <a href="https://github.com/luas13/Leetcode_New"> <img src="img/other_sites/LeetCode.jpg" class="img-responsive" alt="" width="60" height="60"></center></li>
                        </ol>

                    </div>
                </div>
            </div>
            <div class="modal-footer">
              <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
          </div>
      </div>
  </div>
</div>

<!--<section class="no-padding" id="portfolio">-->
<section class="bg-dark" id="portfolio">
    <div class="container-fluid">
        <div class="row no-gutter popup-gallery">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">$ ls Projects</h2>
                <hr class="primary">
            </div>
            <div class="col-lg-4 col-sm-6">
                <a data-toggle="modal" href="#projone" class="portfolio-box" data-target="#projone" type="button">
                    <img src="img/portfolio/thumbnails/1.jpg" class="img-responsive" alt="" >
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Machine Learning
                            </div>
                            <div class="project-name">
                                Simple Neural Network
                            </div>
                            <button type="button" class="btn btn-default">Read more</button>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-lg-4 col-sm-6">
                <a data-toggle="modal" href="#projtwo" class="portfolio-box" data-target="#projtwo" type="button">
                    <img src="img/portfolio/thumbnails/2_5.jpg" class="img-responsive" alt="" >
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Deep Learning- convolution Neural Network
                            </div>
                            <div class="project-name">
                                Facial Expression Recognition
                            </div>
                            <button type="button" class="btn btn-default">Read more</button>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-lg-4 col-sm-6">
                <a data-toggle="modal" href="#projthree" class="portfolio-box" data-target="#projthree" type="button">
                    <img src="img/portfolio/thumbnails/3_1.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Deep Learning- Generative Adversarial Network
                            </div>
                            <div class="project-name">
                                Stereo-pair Generation using GAN
                            </div>
                            <button type="button" class="btn btn-default">Read more</button>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-lg-4 col-sm-6">
                <a data-toggle="modal" href="projfour" class="portfolio-box" data-target="#projfour" type="button">
                    <img src="img/portfolio/thumbnails/4_1.jpg" class="img-responsive" alt="" width="650" height="350">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Machine Learning
                            </div>
                            <div class="project-name">
                                Data Driven representation using PCA and Auto-encoder
                            </div>
                            <button type="button" class="btn btn-default">Read more</button>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-lg-4 col-sm-6">
                <a data-toggle="modal" href="projfive" class="portfolio-box" data-target="#projfive" type="button">
                    <img src="img/portfolio/thumbnails/5_1.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Natural language Processing
                            </div>
                            <div class="project-name">
                                Image Description to Caption
                            </div>
                            <button type="button" class="btn btn-default">Read more</button>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-lg-4 col-sm-6">
                <a data-toggle="modal" href="projsix" class="portfolio-box" data-target="#projsix" type="button">
                    <img src="img/portfolio/thumbnails/6_1.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Multimedia and Web Databases
                            </div>
                            <div class="project-name">
                                Image and Video Search
                            </div>
                            <button type="button" class="btn btn-default">Read more</button>
                        </div>
                    </div>
                </a>
            </div>
        </div>
    </div>
</section>

<!-- project one Modal -->

<div id="projone" class="modal fade" role="dialog">
  <div class="modal-dialog modal-lg">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Simple Neural Network</h4>
    </div>
    <div class="modal-body">

        <div class="panel panel-default">
          <div class="panel-heading" role="tab" id="headingOne">
            <h4 class="panel-title">
                3-Layered Neural Network
            </h4>
        </div>
        <div class="panel-body">
          <!--<p> <h3><strong>Simple Explanation:</strong></h3> An application used to convert text-based guitar tablature into a customizable PDF document.</p>-->
          <p> <h3><strong>Technical Explanation:</strong></h3>
            This project was made to learn basics of Backpropagation algorithm in Neural Networks. I implemented a simple 3-layered neural network to recognize handwritten digits.The model was trained on MNIST dataset.</br>
            The accuracy came around 96%. More tweaks can be done to improve accuracy.</br>
            It uses Python 2.7.</br>
            <ul><li>Project can be found here: <a href="https://github.com/luas13/Machine_Learning/tree/master/3-Layered_Neural_Network" target="_blank"> Github Link </a></li></ul>
        </p>
    </div>
</div>
</div>
<div class="modal-footer">
  <button type="button" class="btn btn-default" data-dismiss="modal" target="_blank">Close</button>
  <a class="btn btn-primary" href="https://github.com/luas13/Machine_Learning/tree/master/3-Layered_Neural_Network" target="_blank" style="background-color: salmon;">Go to Github</a>
</div>
</div>
</div>
</div>

<div id="projtwo" class="modal fade" role="dialog">
  <div class="modal-dialog modal-lg">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Convolution Neural Network</h4>
    </div>
    <div class="modal-body">

        <div class="panel panel-default">
          <div class="panel-heading" role="tab" id="headingOne">
            <h4 class="panel-title">
                Facial Expression Recognition using CNN
            </h4>
        </div>
        <div class="panel-body">
          <!--<p> <h3><strong>Simple Explanation:</strong></h3> An application used to convert text-based guitar tablature into a customizable PDF document.</p>-->
          <p> <h3><strong><u>Technical Explanation:</u></strong></h3>
            There are seven types of human emotions shown to be universally recognizable across different cultures: anger, disgust, fear, happiness, sadness, surprise, contempt. The traditional solution uses the approach that emotions depends on the locations of facial landmarks and that emotions are dependent on the relationship between landmarks, while we use deep learning model CNN to recognise facial expression. Convolutional Neural Networks take advantage of the fact that the input consists of images images and in particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth.</br>
        </br>
        I used FER2013 dataset where I passed the training images to the CNN and through which we obtained   values corresponding to different expressions. Various hyper-parameter optimizations were done to get a good accuracy of 63%. The future work which I intend to do here is use SIFT features along with CNN to get better accuracy. This has been theoritcally shown in a paper. Two other ways were also employed to recognise facial expressions using SVM.</br>
        <p><h2><u>Classification Results</u></h2>
            <center><img src="img/portfolio/proj_2/correct_class.jpg" class="img-responsive" alt="" width="500" height="80"></center>This is correct classification. We got some incorrect classification results too.
            <center><img src="img/portfolio/proj_2/incorrect_class.jpg" class="img-responsive" alt="" width="250" height="50"></center>Few of the samples are classified incorrectly by our model. As shown in the figure above, the input face is clearly a surprised face, but the model classifies it as neutral. 
        </p>
        <p><h2><u>Accuracy Graph</u></h2>
            <img src="img/portfolio/proj_2/training_acc.jpg" class="img-responsive" alt="" width="800" height="60"> <center> Training Accuracy</center>
            This is the training accuracy for CNN for 50 epochs. We can see that the training accuracy is monotonically increasing. This shows that our model is learning the data good.
        </br></br>
        <img src="img/portfolio/proj_2/val_acc.jpg" class="img-responsive" alt="" width="800" height="60"> <center> Validation Accuracy</center> This is the validation accuracy for CNN for 50 epochs. Validation accuracy is increasing with epochs. This proves that the model is not overfitting to the data.
    </p>
    <p><h2><u>Filter in 4th layer</u></h2>
        This shows how filters become abstract layer by layer. We can see how it captures texture features.
        <center><img src="img/portfolio/proj_2/filter_l4.jpg" class="img-responsive" alt="" width="500" height="30"> </center>
        <!--<center> This is how filter looks like for the 4th layer in CNN</center>-->
        <p><h2><u>Confusion Matrix</u></h2>
            The following confusion matrix shows how trained network get confused among different classes. We can find that angry, sad, fear and neutral are easily to be misclassified. Also since we have very little data in disgust dataset, our network cannot classify disgust very well.
            <img src="img/portfolio/proj_2/confusion_mat.jpg" class="img-responsive" alt="" width="800" height="60"> <center> Validation Accuracy</center>
            <p><h2><u>Conclusion</u></h2>
                <img src="img/portfolio/proj_2/approaches.jpg" class="img-responsive" alt="" width="750" height="40"> <center> The 3 approaches used in our project and we found the CNN model performed the best.</center>
                <!--<ul><li>Project can be found here: <a href="https://github.com/luas13/Machine_Learning/tree/master/3-Layered_Neural_Network" target="_blank"> Github Link </a></li></ul>-->
            </p>
        </div>
    </div>
</div>

<div class="modal-footer">
  <button type="button" class="btn btn-default" data-dismiss="modal" target="_blank">Close</button>
  <!--<a class="btn btn-primary" href="https://github.com/luas13/Machine_Learning/tree/master/3-Layered_Neural_Network" target="_blank" style="background-color: salmon;">Go to Github</a>-->
</div>

</div>
</div>
</div>

<div id="projthree" class="modal fade" role="dialog">
  <div class="modal-dialog modal-lg">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Generative Adversarial Network</h4>
    </div>
    <div class="modal-body">

        <div class="panel panel-default">
          <div class="panel-heading" role="tab" id="headingOne">
            <h4 class="panel-title">
                Stereo-pair generation using conditional GAN
            </h4>
        </div>
        <div class="panel-body">
          <!--<p> <h3><strong>Simple Explanation:</strong></h3> An application used to convert text-based guitar tablature into a customizable PDF document.</p>-->
          <p> <h3><strong><u>Technical Explanation:</u></strong></h3>
            A stereo-pair image contains two views of a scene side by side. One of the views is intended for the left eye and the other for the right eye. These images are sometimes viewed with special equipment to direct each eye on to its intended target, but they are also often viewed without equipment. In this research project, we try to generate stereo-pair image for an input image i.e. given a left image we try to generate it's corresponding right image and vice versa. </br></br>
            We approached this problem of stereo-pair generation as a variant of automatic task of translating images from one form to the images of other form, very similar to image to image translation. These 2 images (stereo-pairs) can then be used in a VR or other 3D hardware to generate a 3D image. Lots of work has been done in the area of image to image translation using deep learning architecture. We used idea based on them to come up with a model which can provide a solution to our generation problem. The enormous success of generative models like Variational Auto-encoders and Generative Adversarial Network promises great results and we tried them in our model.</br>
        </br>
        We used KITTI dataset which contains 17gb of high resolution pairs of left and right images with depths.. My model consisted of conditional GAN and an encoder to learn the latent space. 
        <p><h2><u>Model Architecture</u></h2>
            What is GAN? A generative adversarial network (GAN) consists of a generator G and a discriminator D. The discriminator is trained to maximize the probability of assigning the correct label to both training samples and samples generated from generator. While, the generator is trained to minimize the probability of assigning a samples from generator to be fake, so as to generate plausible samples conditioned on the random noise vector z. </br></br>
            To synthesize image conditioned on a given class label c, auxiliary classifier GAN demonstrate how to synthesize high quality image by using new structure and loss function. In the generator G, input the class label c and the noise vector z, a deconvolutional network performs image generation conditioned on both class label c and the noise vector z. The discriminator is a convolutional network, which outputs the class label for the training data, that except requiring generate plausible image, the generator also trained to maximize the log-likelihood of the synthesized image to the correct class. To synthesize an image from another image, the input image need to be projected into latent representation. We use this idea in our model.
            <center><img src="img/portfolio/proj_3/gan1.jpg" class="img-responsive" alt="" width="500" height="70">Training generater and Encoder to learn shared features and project to latent space Z. </br>Step1: the GAN architecture only. </br>Step2: Generater with the encoder learning the latent space</center>
        </br></br>
        <center><img src="img/portfolio/proj_3/gan2.jpg" class="img-responsive" alt="" width="470" height="60">We used trained encoder and generater to predict stereo-pair. Step3</center>
    </p>
    <p><h2><u>Results</u></h2>
        <center><img src="img/portfolio/proj_3/step1.jpg" class="img-responsive" alt="" width="500" height="60">  Output from step1 based on only conditional GAN. It contains the cropped input image and generated images.</center>
    </br></br>
    <center><img src="img/portfolio/proj_3/step2.jpg" class="img-responsive" alt="" width="330" height="35">  It contains the output from Encoder, step2</center>
</br></br>
<center> <img src="img/portfolio/proj_3/step3.jpg" class="img-responsive" alt="" width="330" height="35"> Output from step3- denotes the translated image for the given input image.</center>
</br></br>
The results when 64x64 center cropped images from kiti dataset are used as training data are depicted in above figures. Fig. 3, Fig. 4 and Fig. 5. The training time without GPU was very long, close to two days. When tried running with limited GPU (over Google cloud) but there too the program took around 6-7 hours to complete. We performed 100 epochs for step1 and 30000 epochs for step2. We are unable to conclude any concrete information from our results as the output images are hazy. 
</br></br>
We had our model based on theoretical reasoning which promised the stereo-pair generation. But we were not able to produce results with clear images. All our results contained features of the images. One of the main reason behind this was that our computing power was very less. It took almost half a day to run with a single GPU. The other reason we believe for this is that the difference between the left and right image is very minute and it is very difficult for the generator to learn the difference. Also the parameter and hyper-parameter optimizations could not be done as we thought. 
</p>
<p><h2><u>Conclusion</u></h2>
    We tried implementing a deep learning network with an Auxiliary Conditional Generative Adversarial Network and an Encoder to predict the stereo pair of an image. Our idea was based on an Image translation. We were not able to produce clear image outputs because of computation constraints but we believe that our approach can give better results with more computing power and some parameter optimizations. We gained good knowledge of GAN, other generative models and tensorflow via this project.
</p>
</div>
</div>
</div>

<div class="modal-footer">
  <button type="button" class="btn btn-default" data-dismiss="modal" target="_blank">Close</button>
  <!--<a class="btn btn-primary" href="https://github.com/luas13/Machine_Learning/tree/master/3-Layered_Neural_Network" target="_blank" style="background-color: salmon;">Go to Github</a>-->
</div>

</div>
</div>
</div>

<!-- Project-4 Data driven Representation-->
<div id="projfour" class="modal fade" role="dialog">
  <div class="modal-dialog modal-lg">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">PCA and Auto-encoder</h4>
    </div>
    <div class="modal-body">

        <div class="panel panel-default">
          <div class="panel-heading" role="tab" id="headingOne">
            <h4 class="panel-title">
                Data Driven representation using PCA and Auto-encoder
            </h4>
        </div>
        <div class="panel-body">
          <!--<p> <h3><strong>Simple Explanation:</strong></h3> An application used to convert text-based guitar tablature into a customizable PDF document.</p>-->
          <p> <h3><strong><u>Technical Explanation:</u></strong></h3>
            The project is aimed at creating data-driven representations using principal component analysis (PCA) and sparse auto-encoders with one hidden layer to create representations for images and compare them. We reconstruct the image after encoding the data using both models and report the errors of reconstruction. A classifier is trained on the encoded code word representation of the data from both the models and test data is used to report the accuracy obtained. Structures of PCA and autoencoders are analyzed with a discussion on the different approach they take. Autoencoders and PCA are widely used dimensionality reduction techniques used in real world applications. </br></br>

            The task of this project is to train a PCA model and auto-encoder to learn interesting structures about the data. Using the MNIST dataset and Faces in the wild dataset, we train both the models, encode the images and analyze the structures involved and reconstruction error. Both these models compress the data into lower dimensions and thus rectifies the well known curse of dimensionality problem that we face in real world data.</br></br>

            Further, the encoded images from PCA and autoencoder is used to train a classifier for the MNIST dataset. The encoded test data is used to report the accuracy of the classifier.</br>
        </br>
        We used MNIST and Faces in the Wild datasets.

        <!-- MNIST-->
        <p>
            <h2><u>MNIST: Compressed Code/Structures of PCA and Auto-encoder. Reconstructed images</u></h2>
            
            PCA is a linear transformation technique that reduces the data to its most important components by removing correlated characteristics with minimal loss of information. It emphasizes on the variance among observations. </br></br>

            An auto-encoder neural network is an unsupervised learning algorithm having same number of nodes in the output layer as the number of nodes in the input layer. The auto-encoder consists of two parts, the encoder and the decoder. The task of the encoder is to map the input to a compressed state. The decoder, in turn, decompresses the hidden compressed layer by mapping the hidden layer back to output image</br></br>

            <b><h3>PCA:</h3></b>
            <center><img src="img/portfolio/proj_4/pca_code_750.jpg" class="img-responsive" alt="" width="350" height="50">We visualized the compressed image/feature code generated during PCA</center>
        </br></br>
        <center><img src="img/portfolio/proj_4/Reconstructed_Image.jpg" class="img-responsive" alt="" width="470" height="60">Digit images were re-constructed from the feature code/ PCA structure.</center>

        <b><h3>Sparse Auto-encoder:</h3></b>
        <center><img src="img/portfolio/proj_4/mnist_autoencoder_struct.jpg" class="img-responsive" alt="" width="400" height="60">We visualized the compressed image/feature code generated by auto-encoder</center>
    </br></br>
    <center><img src="img/portfolio/proj_4/mnist_autoencoder_reconstruct_test.jpg" class="img-responsive" alt="" width="470" height="60">Digit images were re-constructed from the feature code/ structure of auto-encoder.</center>
</p>

<p>
    <h2><u>Graphs for PCA on MNIST</u></h2>

    <center><img src="img/portfolio/proj_4/eigenvecVSmse.jpg" class="img-responsive" alt="" width="500" height="60">  Eigen vectors vs mean squared error for training and testing</center>
</br></br>
<center><img src="img/portfolio/proj_4/eigenvecVSpsnr_red_psnr.jpg" class="img-responsive" alt="" width="500" height="60">  Eigen vectors vs psnr error for training and testing</center>
</br></br>
A useful measure to choose optimal number of eigen vectors is called "explained variance", which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components. This is shown is in below fig. The figure shows maximum variance of about 99% is captured by first 330-340 eigen vectors and variance of about 95% by first 155-160 eigen vectors.
<center> <img src="img/portfolio/proj_4/explained_variance_750_ev.jpg" class="img-responsive" alt="" width="500" height="60"> Explained variance vs the eigen vector.</center>
</br></br>
</p>

<!-- Faces in the Wild-->
<p>
    <h2><u>Faces in the Wild: Compressed Structures of PCA and Auto-encoder. Reconstructed images</u></h2>

    <b><h3>PCA:</h3></b>
    <center><img src="img/portfolio/proj_4/lfw_pca_16.jpg" class="img-responsive" alt="" width="530" height="60">We visualized the compressed image/feature code generated during PCA</center>
</br></br>
<center><img src="img/portfolio/proj_4/lfw_pca_reconstruct.jpg" class="img-responsive" alt="" width="470" height="60">Images were re-constructed from the feature code/ PCA structure.</center>

<b><h3>Sparse Auto-encoder:</h3></b>
<center><img src="img/portfolio/proj_4/lfw_autoencoder_struct.jpg" class="img-responsive" alt="" width="700" height="60">We visualized the compressed image/feature code generated by auto-encoder</center>
</br></br>
<center><img src="img/portfolio/proj_4/lfw_autoencoder_reconstruct_test.jpg" class="img-responsive" alt="" width="470" height="60">Images were re-constructed from the feature code/ structure of auto-encoder.</center>
</p>

<p>
    <h2><u>Graphs for PCA on Faces in the Wild</u></h2>
    <center><img src="img/portfolio/proj_4/lfw_mse_plot.jpg" class="img-responsive" alt="" width="500" height="60">  Eigen vectors vs mean squared error for training and testing</center>
</br></br>
<center><img src="img/portfolio/proj_4/lfw_psnr_plot.jpg" class="img-responsive" alt="" width="500" height="60">  Eigen vectors vs psnr error for training and testing</center>
</br></br>
</p>

<p><h2><u>Experimental values</u></h2>
    To normalize our results, we have used a different metric called percentage difference:
    Percentage Difference = [(p1 - p2)/{(p1 + p2)/(2)}] * 100. Below are the some of the figures we obtained.
    <center><img src="img/portfolio/proj_4/pca_analysis.jpg" class="img-responsive" alt="" width="300" height="30">  </center>
</br></br>
<center><img src="img/portfolio/proj_4/auto-encoder_analysis.jpg" class="img-responsive" alt="" width="300" height="30">  </center>
</br></br>

<p><h2><u>Conclusion</u></h2>
    Both PCA and autoencoder reconstruct the images with very less error. The reconstructed images are comparable in nature and not much to discuss on that.</br></br>

    What was interesting was visualizing the structures of PCA and autoencoder. We can see from the previous results that the structures involved in both the models learns from the images and detects structures within them. But there is a difference between their structures. The structures of PCA are smooth and crisp, with every edge and face clearly identifiable by just analyzing them. But the structures of the autoencoders are a bit noisy and it looks like they are hallucinating. </br></br>

    The difference between their structures is primarily because how both models learn their weights. PCA performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in lower-dimensional representation is maximized. This is done by projecting the data on the orthonormal eigenvectors of the covariance matrix. So basically, PCA does a linear mapping of higher-dimensional data to a lower-dimensional data.</br></br>

    On the other hand, an autoencoder, with non-linear activation functions, will do a non-linear mapping of higher-dimensional data to lower-dimensional data. This is the key difference why the structures obtained with PCA and autoencoders differ. This being said, we cannot say that one is better than the other. Both just represents data in a separate way.
</p>
</div>
</div>
</div>

<div class="modal-footer">
  <button type="button" class="btn btn-default" data-dismiss="modal" target="_blank">Close</button>
  <!--<a class="btn btn-primary" href="https://github.com/luas13/Machine_Learning/tree/master/3-Layered_Neural_Network" target="_blank" style="background-color: salmon;">Go to Github</a>-->
</div>

</div>
</div>
</div>


<div id="projfive" class="modal fade" role="dialog">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
          <div class="modal-header">
            <button type="button" class="close" data-dismiss="modal">&times;</button>
            <h4 class="modal-title">Natural Language Processing</h4>
          </div>
        <div class="modal-body">

            <div class="panel panel-default">
              <div class="panel-heading" role="tab" id="headingOne">
                <h4 class="panel-title">
                    Generate captions using Image descriptions
                </h4>
              </div>
            </div>
            <div class="panel-body">
              <!--<p> <h3><strong>Simple Explanation:</strong></h3> An application used to convert text-based guitar tablature into a customizable PDF document.</p>-->
              <p> <h3><strong><u>Technical Explanation:</u></strong></h3>
                In this project, we present a model to solve image captioning problem using semantic graphs from phrases that represents an image. The model is trained such that it captures only the critical words out of all the phrases that can be used to generate the captions. We worked on visual genome(Visual Genome, 2016) dataset for our region phrases that depict images present in COCO dataset(Microsoft coco captions, 2015) and calculate the error using standard BLEU metric measure. We used Simple-NLG to get the captions. We evaluated the captions generated by our model for 500 COCO images using the BLEU-1 metric.</br></br>
                
                Our approach to solve the problem of generating captions From descriptions is a two step approach. In the first step for a given image we generate a Graph for the given set of region descriptions of the image and then try to reduce this graph and extract important nodes and their relations(mod,dobj,nsubj) along with POS tags(later used to predict sentence). Now using these important nodes predict sentence which would be the caption. There are two different approach for this, first using SimpleNLG Sentence Generator to generate sentences using nodes, their relations (from Stanford Dependencies) and POS-tags (using NLTK tokenizer). Second approach is using Recurrent Neural Network based LSTM based model to train captions and nodes obtained and using this model to predict captions from the nodes of test data. We were not able to implement the 2nd approach efficiently and that remains a future task for us.</br>
                </p>
                <p><h2><u>Model Workflow</u></h2>
                    <center><img src="img/portfolio/proj_5/approach.jpg" class="img-responsive" alt="" width="800" height="200">This is the overall flow in our model.</center>
                </p>

                <p><h2><u>Step 1: Stanford Dependencies</u></h2>
                    <center><img src="img/portfolio/proj_5/stanford_dep.jpg" class="img-responsive" alt="" width="800" height="80">We found the POS tags and the universal dependencies using Stanford parser.</center>
                </p>
                <p><h2><u>Step 2: Generate Graph</u></h2>
                    We started by generating a graph(G1) of the descriptions where nodes of graph are nouns, verbs and adjectives, we used nltk tokenizer for detecting this. Edges are obtained by feeding sentences to Stanford Parser one by one and then from the parsed dependencies we keep only those which have noun, verbs and adjectives. While adding nodes to the graph we used lemmetization technique to make sure that nodes are not duplicated with same context.
                    <center><img src="img/portfolio/proj_5/generate_graph.jpg" class="img-responsive" alt="" width="800" height="80"></center>
                    We also generated a graph(G2) using Stanford Parser Dependencies which contains relations on the edges of the Graph. Relations preserved are nsubj, dobj and acl. This graph is later used to feed POS tags and dependencies to SimpleNLG.
                </p>
                <p><h2><u>Step 3: Reduce Graph by finding important nodes using centrality</u></h2>
                    We ranked these nodes based on standard centrality measures such as eigen centrality, eigen Katz centrality, betweenness centrality, degree centrality and assoc space centrality. Here Assoc Space centrality takes care of the external knowledge as it is derived from ConceptNet graph matrix. 
                    <center><img src="img/portfolio/proj_5/centrality.jpg" class="img-responsive" alt="" width="800" height="80">Various centrality were used to find important nodes.</center>
                    We tried to learn the weights(w1,w2,w3,w4,w5) for these centralities so that we can predict nodes efficiently. We tried to maximize the node strength of important nodes (intuitively present in  original captions). To achieve this task we defined a custom loss function and trained weights to achieve better accuracy by minimizing the loss function. After learning the weights we applied the same centrality measures to test data and extract important nodes based on these centrality measures.
                    <center><img src="img/portfolio/proj_5/weight_centrality.jpg" class="img-responsive" alt="" width="320" height="60">Estimating weights</center>
                    </br></br>
                    <center><img src="img/portfolio/proj_5/reduce_graph.jpg" class="img-responsive" alt="" width="800" height="80">Reducing graph</center>
                </p>
                <p><h2><u>Step 4: Generating Sentences</u></h2>
                    Considering top 20 Nodes based on centrality measures, the important task was to generate sentences using these nodes. We followed two different approaches to generate sentences. In the first approach we used SimpleNLG, we used graph G2 and important nodes from centrality measures to feed them as input to SimpleNLG sentence generator. SimpleNLG takes verb, object and subject as input and outputs the sentence. We are able to achieve some accuracy using this approach.
                    <center><img src="img/portfolio/proj_5/2_approaches.jpg" class="img-responsive" alt="" width="800" height="80">Sentence generation using simple NLG</center>

                    Second approach was based on Neural Network LSTM based model where idea is to generate sentences using neural network where we feed the tuple of important nodes and the COCO captions in the training phase to an LSTM (Long Short Term Memory) Model and it will intuitively learn tuple 
                    relation. After we train this model we create another model which takes in context the word em-
                    beddings and predict the sentence using the first LSTM model. We could not implement this efficiently.
                    <center><img src="img/portfolio/proj_5/lstm.jpg" class="img-responsive" alt="" width="320" height="60">Sentence generation using simple NLG</center>
                </p>
                <p><h2><u>Results</u></h2>
                    <center><img src="img/portfolio/proj_5/results.jpg" class="img-responsive" alt="" width="600" height="55">  Results of caption generation using simple NLG. The BLEU score we got was 0.07</center>
                    </br></br>
                    <center> <img src="img/portfolio/proj_5/final_weights.jpg" class="img-responsive" alt="" width="320" height="60"> We also show below the final weights learned.</center>
                </p>

                </br></br>
                <p> <h3><strong><u>Conclusion:</u></strong></h3>
                We experimented and learned various NLP and machine learning techniques throughout this project. We learned POS tagging and tokenizing using NLTK Tokenizer we got all the POS tags for words. We also learned lemmatization using TextBlob lemmatization to get word root before adding node to the graph so that multiple nodes are not inserted for one context. We extracted universal dependencies using Stanford Parser to find dobj, nsubj and acl relations between words that are used in sentence generation and generation of graph. We learned about centrality measures such eigen centrality, degree centrality, eigen-katz centrality and betweenness centrality to capture node potential in graph. To use external knowledge base we used Assoc Space Matrix centrality which is based on ConceptNet relations. We learned Word2Vec to train our own word2vec model and also worked on using pre-defined trained model as an experiment. We defined custom loss function to learn weights for all the centrality measures. We learned loss function optimization using scikit lmfit function to optimize weights based on our custom loss function minimization. To generate the sentences we learned Simple NLG which takes verb, subject and object as input and generates the template based sentences. We need to learn more on training LSTM model to generate captions from words. This is a future task for us.
                </p>
            </div>

        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal" target="_blank">Close</button>
          <!--<a class="btn btn-primary" href="https://github.com/luas13/Machine_Learning/tree/master/3-Layered_Neural_Network" target="_blank" style="background-color: salmon;">Go to Github</a>-->
        </div>
        </div>

    </div>
</div>
</div>
    <!--
    <aside class="bg-dark">
        <div class="container text-center">
            <div class="call-to-action">
                <h2>Free Download at Start Bootstrap!</h2>
                <a href="http://startbootstrap.com/template-overviews/creative/" class="btn btn-default btn-xl sr-button">Download Now!</a>
            </div>
        </div>
    </aside>
-->

<section class="bg-dark" id="cv">
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">Resume</h2>
                <div style="text-align: center;"><iframe height="900" width="85%" src="https://www.edocr.com/embed/y9xoraz4"></iframe><p style="text-align: center; font-size: .8em;"></p>
                </div>
            </div>
            <div class="call-to-action text-center">
                <a href="https://www.edocr.com/web-api/shares/y9xoraz4/download?redirect=false" class="btn btn-primary btn-xl sr-button">Download</a>
            </div>
        </div>
    </div>
</section>

<section class="bg-dark" id="contact">
    <div class="container">
        <!--<aside class="bg-dark">-->
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 text-center">
                <h2 class="section-heading">Well, this is me.</h2>
                <hr class="primary">
                <!--<p>Ready to start your next project with us? That's great! Give us a call or send us an email and we will get back to you as soon as possible!</p>-->
                <center> <img src="img/portfolio/thumbnails/asu_dp.jpg" class="img-responsive" alt=""> </center>
                <p>Ready to talk <a color:transparent><font size="7"> &#9786; </font></a> Let's Get In Touch</p>
            </div>
                <!--
                <div class="col-lg-4 col-lg-offset-2 text-center">
                    <i class="fa fa-phone fa-3x sr-contact"></i>
                    <p>********</p>
                </div>
            -->
            <div class="col-lg-4 text-center">
                <i class="fa fa-envelope-o fa-3x sr-contact"></i>
                <p><a href="mailto:your-email@your-domain.com">abhishek.kr@asu.edu</a></p>
            </div>
            <!--<div class="col-lg-4 col-lg-offset-2 text-center">-->
            <div class="col-lg-4 text-center">
                <i class="fa fa-linkedin-square fa-3x sr-contact"></i>
                <p><a href="https://www.linkedin.com/in/abhishek-kumar-asu/" target="blank">LinkedIn</a></p>
            </div>
            <div class="col-lg-4 text-center">
                <i class="fa fa-github fa-3x sr-contact"></i>
                <p><a href="https://github.com/luas13" target="blank">Github</a></p>
            </div>
        </div>
        <!--</aside>-->
    </div>
</section>

<!-- jQuery -->
<script src="vendor/jquery/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="vendor/bootstrap/js/bootstrap.min.js"></script>

<!-- Plugin JavaScript -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
<script src="vendor/scrollreveal/scrollreveal.min.js"></script>
<script src="vendor/magnific-popup/jquery.magnific-popup.min.js"></script>

<!-- Theme JavaScript -->
<script src="js/creative.min.js"></script>


<footer>
    <div class="container-fluid" style='margin-left:15px' position="fixed" >
        <!--<p><a href="https://www.linkedin.com/in/abhishek-kumar-asu/" target="blank">LinkedIn</a> | <a href="https://github.com/luas13" target="blank">Github</a></p>-->
        <center><p>Made with <a color:transparent> &#9829; </a> by Abhishek</p></center>
    </div>
</footer>

</body>

</html>